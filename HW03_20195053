from google.colab import drive
drive.mount('/content/drive')

''' Step 1 '''
# Import packages
import os
import torch
import numpy as np
import torch.nn.functional as F
import torchvision
import torchvision.transforms
import matplotlib.pyplot as plt
import sys
from PIL import Image
from pycocotools.coco import COCO
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

''' Step 2 '''
# Build dataset using pycocotools api
class FaceMaskDataset(torch.utils.data.Dataset): 
    def __init__(self, json_path, img_path):
        self.json_path = json_path
        self.img_path = img_path
        self.coco = COCO(json_path)
        self.image_ids = list(self.coco.imgToAnns.keys())

    def __len__(self): 
        return len(self.image_ids)

    def __getitem__(self, idx):       
        image_id = self.image_ids[idx]
        file_name = self.coco.loadImgs(image_id)[0]['file_name']
        image = Image.open(os.path.join(self.img_path, file_name)).convert('RGB')


        annot_ids = self.coco.getAnnIds(imgIds=image_id) # call annotation ids by using image_id 
        annots = [x for x in self.coco.loadAnns(annot_ids) if x['image_id'] == image_id] # call annotations by using annotation id 

    # convert bounding box (x, y, w, h) -> (x1, y1, x2, y2)
        boxes = np.array([annot['bbox'] for annot in annots], dtype=np.float32) 
        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]
        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]

        labels = np.array([annot['category_id'] for annot in annots], dtype=np.int32) # determine object wear mask correctly or not
        iscrowd = np.array([annot['iscrowd'] for annot in annots], dtype=np.uint8) #  whether the annotation is for the single object (0) or for the multiple objects
        
        
        target = {
            'image_id': image_id,
            'boxes': boxes,
            'labels': labels,
            'iscrowd': iscrowd,
        }
          
    
    # convert numpy array to torch tensor and define data type of tensor
        target["image_id"] = torch.as_tensor(target['image_id'], dtype=torch.uint8)
        target['boxes'] = torch.as_tensor(target['boxes'], dtype=torch.float32)
        target['labels'] = torch.as_tensor(target['labels'], dtype=torch.int64)
        target['iscrowd'] = torch.as_tensor(target['iscrowd'], dtype=torch.uint8)   

        transform = torchvision.transforms.Compose([
                                                    torchvision.transforms.ToTensor()
                                                    ])#Resize 함수가 resnet50_fpn에 있기 때문에 Transform에 Resize를 추가할 필요가 없다.
        
        img = transform(image)

    # return image, target 
        return img, target

  # ....
  
  ''' Step 3 ''' 
# Build dataloader for train, validation and test dataset

# Define collate_fn 
def collate_fn(batch):
    return tuple(zip(*batch))

# For instance, 
batch_size =  2 

# Train
train_json_path =  '//content/drive/MyDrive/2022_deep_learning/coding_lecture/Homework/HW03/FaceMask_Detection_Dataset/coco_json/train.json'
img_path =  '//content/drive/MyDrive/2022_deep_learning/coding_lecture/Homework/HW03/FaceMask_Detection_Dataset/images/'
train_dataset = FaceMaskDataset(train_json_path, img_path)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn = collate_fn)

# Test 
test_json_path = '//content/drive/MyDrive/2022_deep_learning/coding_lecture/Homework/HW03/FaceMask_Detection_Dataset/coco_json/test.json'
test_dataset = FaceMaskDataset(test_json_path, img_path)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn = collate_fn)

# Validation 
val_json_path = '//content/drive/MyDrive/2022_deep_learning/coding_lecture/Homework/HW03/FaceMask_Detection_Dataset/coco_json/val.json'
val_dataset = FaceMaskDataset(val_json_path, img_path)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn = collate_fn)

''' Step 1 '''
# Load model for object detection
import torch 
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

model 

''' Step 2 '''
# Modify the architecture of model to fit our data
# !Notice! Number of the class of the dataset is three, but we have to consider that we need one more class for "background(0)"
num_classes = 4

in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
model

''' Step 1 '''
# Define hyper-parameters for training
import torch.nn as nn

device ="cpu"
 
model = model.to(device)

params = [p for p in model.parameters() if p.requires_grad]

optimizer =  torch.optim.SGD(params, lr=0.001,momentum=0.9, weight_decay=0.0005)

lr_scheduler =  torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1) 

criterion = nn.CrossEntropyLoss()
# ....


''' Step 2 '''
import sys
sys.path.append('//content/drive/MyDrive/2022_deep_learning/coding_lecture/05.19.CodingLecture7/vis_utils')
import utils
from engine import *
from coco_eval import CocoEvaluator
from coco_utils import get_coco_api_from_dataset
import pdb

# Define function for training
'''def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):
    model.train()
    # Record Learning rate 
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value:.6f}")) 
    header = f"Epoch: [{epoch}]"

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )
    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        print('targets',targets[0]['labels'])
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        
        with torch.cuda.amp.autocast(enabled=scaler is not None):
            output = model(images)
            loss = criterion(output, label)
            losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return metric_logger'''
    
def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):
    model.train()
    # Record Learning rate 
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value:.6f}")) 
    header = f"Epoch: [{epoch}]"

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        for j in range(batch_size):
            #print('target label is',targets[j]['labels'],targets[j]['labels'].shape)
            answer = F.one_hot(targets[j]['labels'])
            #print('answer is ',answer)
            with torch.cuda.amp.autocast(enabled=scaler is not None):
                loss_dict = model(images, targets)
                #output = model(images)
                print('output is',loss_dict)
                losses = sum(loss for loss in loss_dict.values())

        # Record Losses 
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        # Sum each values of losses
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())

        loss_value = losses_reduced.item()

        if not math.isfinite(loss_value):
            print(f"Loss is {loss_value}, stopping training")
            print(loss_dict_reduced)
            sys.exit(1)

        optimizer.zero_grad()
        if scaler is not None:
            scaler.scale(losses).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            losses.backward()
            optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])

    return metric_logger
    
    ''' Step 3 '''
# Define function for evaluation
sys.path.append('//content/drive/MyDrive/2022_deep_learning/coding_lecture/05.19.CodingLecture7/vis_utils')
import utils 
from engine import *

def evaluate(model, data_loader, device):
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ")
    header = "Test:"

    for images, targets in metric_logger.log_every(data_loader, 100, header):
        images = list(img.to(device) for img in images)
        outputs = model(images)
        outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]
        res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}
        print(res)
    # gather the stats from all processes
    print("Averaged stats:", metric_logger)
    return 1


''' Step 4 '''
# Training loop
# num_epochs <= 10
num_epochs = 10

for epoch in range(num_epochs): 
  train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=10)
  lr_scheduler.step()
  
  ''' Step 5 '''
# Evaluate on the test dataset 
evaluate(model, test_dataloader, device=device)

''' Step 6 '''
# Inference on test images and visualize results.
import cv2
def overlay_instances(img, prediction, threshold=0.8):
    label_dict = {0 : 'background', 1: 'person'}
    ori_img = img.mul(255).permute(1,2,0).byte().numpy() # Tensor * 255, Convert RGB -> BGR, Tensor to numpy
    for idx in range(len(prediction[0]['boxes'])):
        score = prediction[0]['scores'][idx].cpu().detach().numpy().item() # detach() : Generation of tensors that do not propagate gradients from existing sensors
        if score < threshold: 
            continue
        box = x1,y1,x2,y2 = prediction[0]['boxes'][idx].cpu().detach().numpy()
        label = prediction[0]['labels'][idx].cpu().detach().numpy().item()
        cv2.rectangle(ori_img, (x1, y1), (x2, y2), (255,0,0),2) 
        cv2.rectangle(ori_img, (int(x1), int(y1)), (int(x1+60), int(y1+20)), (255,0,0),-1)
        #cv2.putText(ori_img, label_dict[label], (int(x1), int(y1+15)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), thickness=1)
    return Image.fromarray(ori_img)

# idxs = list of indexes
idxs = [10, 11, 12]
for idx in idxs:
    img, _ = val_dataset[idx] # idx = index of image
    model.eval() 
    with torch.no_grad(): 
        prediction = model([img.to(device)])
    results = overlay_instances(img, prediction, threshold=0.8)
    plt.axis('off')
    plt.imshow(results)
    plt.show()
